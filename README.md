<div align="center">

<img src="https://raw.githubusercontent.com/InternLM/InternLM/main/assets/logo.svg" width="200"/>
  <div> </div>
  <div align="center">
    <b><font size="5">InternLM2-WQX</font></b>
    <sup>
      <a href="https://internlm.intern-ai.org.cn/">
        <i><font size="4">HOT</font></i>
      </a>
    </sup>
    <div> </div>
  </div>

[![license](https://raw.githubusercontent.com/InternLM/InternLM/main/assets/license.svg)](./LICENSE)



InternLM2-WQX-20B <a href="https://huggingface.co/internlm/internlm2-wqx-20b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-wqx-20b/summary"><img src="./assets/modelscope_logo.png" width="20px"></a> ï½œ InternLM2-WQX-VL-20B <a href="https://huggingface.co/internlm/internlm2-wqx-vl-20b">ğŸ¤—</a> <a href="https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-wqx-vl-20b/summary"><img src="./assets/modelscope_logo.png" width="20px"></a>
</div>

# Introduction

InternLM2-WQXä¸InternLM2-WQX-VLæ˜¯InternLMå›¢é˜Ÿäº2024å¹´é«˜è€ƒå‰å¤•æœ€æ–°æ¨å‡ºçš„æ–‡æ›²æ˜Ÿç³»åˆ—æ¨¡å‹ã€‚

é«˜è€ƒè¦†ç›–å„ç±»å­¦ç§‘åŠé¢˜å‹ï¼ŒåŒæ—¶å› å…¶å¼€è€ƒå‰çš„â€œç»å¯†æ€§â€ï¼Œè¢«è§†ä½œä¸­å›½æœ€å…·æƒå¨çš„è€ƒè¯•ä¹‹ä¸€ï¼Œæˆä¸ºè¯„ä¼°è€ƒç”Ÿç»¼åˆèƒ½åŠ›çš„â€œè¯•é‡‘çŸ³â€ã€‚è¿™ä¸€é¢å‘äººç±»è®¾è®¡çš„é«˜éš¾åº¦ç»¼åˆæ€§æµ‹è¯•ï¼Œç›®å‰æ™®éè¢«ç ”ç©¶è€…ç”¨äºè€ƒå¯Ÿå¤§æ¨¡å‹çš„æ™ºèƒ½æ°´å¹³ã€‚InternLM2-WQXç³»åˆ—æ¨¡å‹åœ¨2024å¹´é«˜è€ƒè¯„æµ‹é›†[GAOKAO-Eval](https://github.com/open-compass/GAOKAO-Eval)ä¸Šå–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œç»¼åˆè¡¨ç°ä¸GPT-4oç›¸å½“ï¼Œä¸”è¶…è¶Šäº†å›½å†…å¤–ä¸€ç³»åˆ—å¼€æºå¤§æ¨¡å‹ï¼Œä½“ç°äº†InternLM2-WQXç³»åˆ—æ¨¡å‹ä¼˜ç§€çš„æ€§èƒ½ã€‚

æˆ‘ä»¬å³å°†æ›´æ–°å…³äºæ–‡æ›²æ˜Ÿç³»åˆ—æ¨¡å‹æ•°æ®å‡†å¤‡çš„ç›¸å…³è¯´æ˜ï¼Œæ•¬è¯·æœŸå¾…ã€‚


# Model Zoo


| Model                       | HuggingFace                          | ModelScope                           | Release Date |
| --------------------------- | ----------------------------------------- | ---------------------------------------- | ------------ |
| **InternLM2-WQX-20B**          | [ğŸ¤—internlm2-wqx-20b](https://huggingface.co/internlm/internlm2-wqx-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-wqx-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-wqx-20b/summary) | 2024-06-04   |
| **InternLM2-WQX-VL-20B**          | [ğŸ¤—internlm2-wqx-vl-20b](https://huggingface.co/internlm/internlm2-wqx-vl-20b) | [<img src="./assets/modelscope_logo.png" width="20px" /> internlm2-wqx-vl-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm2-wqx-vl-20b/summary) | 2024-06-04   |


## MD5 Check

### LLMæƒé‡æ–‡ä»¶çš„md5å€¼
```
md5sum ./*
5209adfd6ef7d1724848ff0372362568  ./model-00001-of-00004.safetensors
e37ee2eafecfed543d10dca75998204e  ./model-00002-of-00004.safetensors
ea3da8035b0c2a31c369dd463adf9b52  ./model-00003-of-00004.safetensors
f1ff218f801c69fd4c12c534b64e1b60  ./model-00004-of-00004.safetensors
```

### MLLMæƒé‡æ–‡ä»¶çš„md5å€¼
```
md5sum ./*
158657dbae9bc369d67cf4bfbdfaaf71  ./pytorch_model-00001-of-00005.bin
c21db8ac1315c10df768f6c3ae3f2825  ./pytorch_model-00002-of-00005.bin
ebc4b0b70e8e9f1adc0b728558d650fb  ./pytorch_model-00003-of-00005.bin
eaa393a66dc632d0a6f0f7d815c439bb  ./pytorch_model-00004-of-00005.bin
7e6e3237d99a7e8bd7ca9ba10747bfdb  ./pytorch_model-00005-of-00005.bin

./clip_l_560_pro7b/*
97b05f40ee9826eda467489eed65f85c  ./clip_l_560_pro7b/pytorch_model.bin
```

# Quick Start

### å¿«é€Ÿè°ƒç”¨**InternLM2-WQX-20B**è¯­è¨€æ¨¡å‹

ä½¿ç”¨transformers åç«¯è¿›è¡Œæ¨ç†

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

device = "cuda"

tokenizer = AutoTokenizer.from_pretrained("internlm/internlm2-wqx-20b", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    "internlm/internlm2-wqx-20b",
    torch_dtype=torch.bfloat16,
    trust_remote_code=True
).to(device).eval()

query = "å·²çŸ¥åœ†æŸ±å’Œåœ†é”¥çš„åº•é¢åŠå¾„ç›¸ç­‰ï¼Œä¾§é¢ç§¯ç›¸ç­‰ï¼Œä¸”å®ƒä»¬çš„é«˜å‡ä¸º$ \\sqrt { 3 }$ï¼Œåˆ™åœ†é”¥çš„ä½“ç§¯ä¸ºï¼ˆ ï¼‰ï¼\nA. $ 2 \\sqrt { 3 } \\pi$\nB. $ 3 \\sqrt { 3 } \\pi$\nC. $ 6 \\sqrt { 3 } \\pi$\nD. $ 9 \\sqrt { 3 } \\pi$"

inputs = tokenizer(query, return_tensors="pt")

inputs = inputs["input_ids"].to(device)

gen_kwargs = {"max_length": 1024, "do_sample": False}

outputs = model.generate(inputs, **gen_kwargs)
outputs = outputs[0].cpu().tolist()[len(inputs[0]) :]

response = tokenizer.decode(outputs, skip_special_tokens=True)
print(response)
```

ä½¿ç”¨vllm åç«¯è¿›è¡Œæ¨ç†ï¼š

```python
from vllm import LLM, SamplingParams

model_name = "internlm/internlm2-wqx-20b"
prompts = ["å·²çŸ¥åœ†æŸ±å’Œåœ†é”¥çš„åº•é¢åŠå¾„ç›¸ç­‰ï¼Œä¾§é¢ç§¯ç›¸ç­‰ï¼Œä¸”å®ƒä»¬çš„é«˜å‡ä¸º$ \\sqrt { 3 }$ï¼Œåˆ™åœ†é”¥çš„ä½“ç§¯ä¸ºï¼ˆ ï¼‰ï¼\nA. $ 2 \\sqrt { 3 } \\pi$\nB. $ 3 \\sqrt { 3 } \\pi$\nC. $ 6 \\sqrt { 3 } \\pi$\nD. $ 9 \\sqrt { 3 } \\pi$"]
sampling_params = SamplingParams(temperature=0.0, max_tokens=1024)

llm = LLM(
    model=model_name,
    trust_remote_code=True,
    enforce_eager=True,
)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, \nGenerated text: {generated_text!r}")
```

### **InternLM2-WQX-20B**è¯­è¨€æ¨¡å‹çš„ Web UI

ä½¿ç”¨transformersåç«¯è¿›è¡Œæ¨ç†ï¼š

```
python basic_demo/web_ui_wqx.py -m internlm/internlm2-wqx-20b
```

### å¿«é€Ÿè°ƒç”¨**InternLM2-WQX-VL-20B**è§†è§‰è¯­è¨€æ¨¡å‹

ä½¿ç”¨transformersåç«¯è¿›è¡Œæ¨ç†:

```python
from PIL import Image
from io import BytesIO
import requests
from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM
import torch
from infer_wqx_vl import process_query_and_image, HD_transform

model_path = "internlm/internlm2-wqx-vl-20b"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda().eval()
model.cuda().half()
model.tokenizer = tokenizer

image_url = "https://ks-1302698447.cos.ap-shanghai.myqcloud.com/img/phymerge.png"
query = "ä½“è‚²è¯¾ä¸Šä¸¤ä½åŒå­¦åœ¨å®¤å†…ç¾½æ¯›çƒåœºè¿›è¡Œç¾½æ¯›çƒæ¯”èµ›ï¼Œç¾½æ¯›çƒåœ¨ç©ºä¸­ä¸Šå‡çš„è¿åŠ¨è½¨è¿¹å¦‚å›¾ä¸­è™šçº¿æ‰€ç¤ºï¼Œè€ƒè™‘ç©ºæ°”é˜»åŠ›ï¼Œç¾½æ¯›çƒåŠ é€Ÿåº¦æ–¹å‘ç¤ºæ„å›¾å¯èƒ½æ­£ç¡®çš„æ˜¯ï¼ˆ\u3000\u3000ï¼‰ \nA:<IMAGE 0>  \nB: <IMAGE 1>  \nC:<IMAGE 2>  \nD:<IMAGE 3> "

response = requests.get(image_url)
image = Image.open(BytesIO(response.content))
embeds, im_mask = process_query_and_image(query, image, model, HD_transform)

outputs = model.generate(inputs_embeds=embeds, im_mask=im_mask,
                            temperature=0.0, max_new_tokens=256, num_beams=1,
                            do_sample=False, repetition_penalty=1.0)
output_token = outputs[0]
output_text = model.tokenizer.decode(output_token, add_special_tokens=False)
print(output_text)
#  <s> æ–œå‘ä¸‹
# ç­”æ¡ˆæ˜¯ï¼šC</s>
```
é’ˆå¯¹è¿™ä¸ªé€‰é¡¹é‡Œé¢æœ‰å›¾ç‰‡çš„è€ƒé¢˜ï¼Œæˆ‘ä»¬å°†å›¾ç‰‡è¿›è¡Œäº†åˆå¹¶å¹¶æ ‡è®°ä¸Š`<IMAGE {id}>`æ¥è®©è¯­è¨€æ¨¡å‹èƒ½ç†è§£å¤šå›¾è€ƒé¢˜ã€‚ å½“å‰ç¤ºä¾‹å±•ç¤ºçš„æ˜¯å·²ç»æ‹¼æ¥å¥½çš„å›¾ç‰‡ï¼Œè¯¦ç»†çš„å›¾åƒé¢„å¤„ç†è¯·å‚è€ƒ[GAOKAO-Eval](https://github.com/open-compass/GAOKAO-Eval)ä¸­çš„å¤šæ¨¡æ€å¤„ç†å·¥å…·ã€‚

### **InternLM2-WQX-VL-20B**è¯­è¨€æ¨¡å‹çš„ Web UI

ä½¿ç”¨transformersåç«¯è¿›è¡Œæ¨ç†ï¼š

```
python web_ui_wqx_vl.py -m internlm/internlm2-wqx-vl-20b
```

# Citation

```bibtex
@misc{2024internlm2wqx,
    title={https://github.com/InternLM/InternLM-WQX},
    author={InternLM Team},
    howpublished = {\url{https://github.com/InternLM/InternLM-WQX}},
    year={2024}
}
```